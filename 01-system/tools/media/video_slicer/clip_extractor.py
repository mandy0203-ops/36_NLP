#!/usr/bin/env python3
"""
AI Video Slicer - æ™ºæ…§å½±ç‰‡åˆ‡ç‰‡å·¥å…· (Class Version)
ç”¨é€”: è®€å–å­—å¹•æª”ï¼Œåˆ©ç”¨ LLM åˆ†æç²¾å½©ç‰‡æ®µï¼Œä¸¦è‡ªå‹•åˆ‡å‰²å½±ç‰‡ã€‚
"""

import os
import sys
import json
import yaml
import subprocess
from pathlib import Path
from typing import List, Dict, Optional

# Add project root to path
sys.path.append(str(Path(__file__).parents[4]))

class VideoSlicer:
    def __init__(self, config_path: str = None, api_key: str = None):
        self.base_dir = Path(__file__).parent
        
        if config_path:
            self.config = self._load_config(config_path)
        else:
            self.config = self._load_config(self.base_dir / "config.yaml")
            
        # Support multiple API keys for rotation
        if api_key:
            self.api_keys = [api_key] if isinstance(api_key, str) else api_key
        else:
            self.api_keys = self._load_api_key(self.config['llm']['provider'])
            
        # Load Style Guide
        self.style_guide = self._load_style_guide(self.base_dir / "style_guide.yaml")

    def _load_style_guide(self, path):
        if path.exists():
            with open(path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        return {}

    def _load_config(self, config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _load_api_key(self, provider):
        """Load API keys (supports multiple keys for rotation)"""
        key_file = Path(__file__).parents[4] / "01-system/configs/apis/API-Keys.md"
        if not key_file.exists():
            raise FileNotFoundError(f"éŒ¯èª¤: æ‰¾ä¸åˆ° API Key è¨­å®šæª”: {key_file}")
            
        keys = []
        with open(key_file, 'r', encoding='utf-8') as f:
            for line in f:
                if provider == "gemini" and line.startswith("GEMINI_API_KEY"):
                    key = line.split("=", 1)[1].strip()
                    if key and not key.startswith("sk-..."):  # Skip placeholder
                        keys.append(key)
                elif provider == "openai" and line.startswith("OPENAI_API_KEY"):
                    key = line.split("=", 1)[1].strip()
                    if key and not key.startswith("sk-..."):
                        keys.append(key)
                elif provider == "claude" and line.startswith("ANTHROPIC_API_KEY"):
                    key = line.split("=", 1)[1].strip()
                    if key and not key.startswith("sk-..."):
                        keys.append(key)
        
        if not keys:
            raise ValueError(f"éŒ¯èª¤: åœ¨ {key_file} ä¸­æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ {provider.upper()}_API_KEY")
        
        # Return all keys for rotation
        return keys

    def parse_srt(self, srt_path):
        """ç°¡å–®è§£æ SRT æª”æ¡ˆï¼Œå›å‚³ç´”æ–‡å­—å…§å®¹ (å¼·åˆ¶è½‰ç¹é«”)"""
        import opencc
        cc = opencc.OpenCC('s2t')  # ç°¡é«”è½‰ç¹é«”
        
        text_content = ""
        with open(srt_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        for line in lines:
            line = line.strip()
            if not line: continue
            if line.isdigit(): continue
            if "-->" in line: continue
            
            # è½‰ç¹é«”
            line = cc.convert(line)
            text_content += line + " "
            
        return text_content

    def analyze_transcript(self, text):
        """ä½¿ç”¨ LLM åˆ†æå­—å¹•å…§å®¹ - ä¸»é¡Œå¼åˆ‡åˆ†"""
        print("ğŸ¤– AI (å…§å®¹åˆ†æå¸«æ¨¡å¼) æ­£åœ¨æ·±åº¦åˆ†æå­—å¹•...")
        
        provider = self.config['llm']['provider']
        
        if provider == "claude":
            return self._analyze_with_claude(text)
        elif provider == "gemini":
            return self._analyze_with_gemini(text)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ LLM provider: {provider}")
    
    def _analyze_with_claude(self, text):
        """ä½¿ç”¨ Claude API åˆ†æ"""
        from anthropic import Anthropic
        
        # Construct Prompt from Style Guide
        style = self.style_guide
        
        hooks_str = ""
        for hook in style.get('hooks', []):
            hooks_str += f"    *   **{hook['category']}**ï¼š\n"
            for ex in hook['examples']:
                hooks_str += f"        - {ex}\n"
                
        constraints_str = "\n".join([f"- {item}" for item in style.get('negative_constraints', {}).get('items', [])])
        positive_str = "\n".join([f"{i+1}. {item}" for i, item in enumerate(style.get('positive_examples', {}).get('items', []))])
        
        prompt = f"""
## åˆ†æç›®æ¨™ï¼šè¡ŒéŠ·çŸ­å½±éŸ³ç´ æï¼ˆTeasersï¼‰

æˆ‘å€‘éœ€è¦çš„æ˜¯ã€Œè²©å”®èª²ç¨‹ã€çš„è¡ŒéŠ·ç´ æã€‚é¢¨æ ¼åƒè€ƒã€Œ{style.get('marketing_strategy', {}).get('style_reference', 'å‚‘æ‰£èŠæºé€š')}ã€ã€‚
ç­–ç•¥æ˜¯**ã€Œ{style.get('marketing_strategy', {}).get('goal', '')}ã€**ã€‚
è«‹æ‰¾å‡ºèƒ½å¼•ç™¼è§€çœ¾ã€Œ{style.get('marketing_strategy', {}).get('target_audience_feeling', '')}ã€çš„å®Œæ•´æ®µè½ã€‚

## æŒ‘é¸æ¨™æº–ï¼ˆä¸‰è¦ç´ ï¼‰

è«‹å°‹æ‰¾åŒ…å«ä»¥ä¸‹ä»»ä¸€ï¼ˆæˆ–å¤šå€‹ï¼‰è¦ç´ çš„æ®µè½ï¼š
1.  **ç—›é» (Pain Points)**ï¼šå¸¸è¦‹çš„æºé€šç½é›£ã€‚
2.  **é‰¤å­ (Hooks) - åƒè€ƒçˆ†æ¬¾é‚è¼¯**ï¼š
{hooks_str}
3.  **æƒ…ç·’ (Emotion)**ï¼šèƒ½æŒ‘å‹•æƒ…ç·’çš„æ®µè½ã€‚

## é—œéµè¦æ±‚ï¼šæ•˜äº‹å®Œæ•´æ€§ (Narrative Completeness)

*   **æœ€é‡è¦**ï¼šæ¯å€‹ç‰‡æ®µå¿…é ˆæ˜¯**ã€Œæœ‰é ­æœ‰å°¾çš„å®Œæ•´æ•˜äº‹ã€**ã€‚
*   **ä¸é™æ™‚é•·**ï¼šåªè¦æ•…äº‹å®Œæ•´ï¼Œé•·åº¦ä¸é™ï¼ˆä½†é€šå¸¸å»ºè­°åœ¨ 30ç§’ åˆ° 3åˆ†é˜ä¹‹é–“ï¼‰ã€‚
*   **çµæ§‹å®Œæ•´**ï¼šå¿…é ˆåŒ…å«ã€Œæƒ…å¢ƒ/å•é¡Œ -> ç™¼å±•/è½‰æŠ˜ -> æš«æ™‚çš„çµè«–/æ‡¸å¿µã€ã€‚ä¸è¦åˆ‡åœ¨è©±è¬›ä¸€åŠçš„åœ°æ–¹ã€‚

**ç¬¬äºŒæ­¥ï¼šä¸»é¡Œåˆ‡åˆ†ï¼ˆ{style.get('quality_control', {}).get('principle', 'å¯§ç¼ºå‹¿æ¿«')}ï¼‰**
è«‹**æœ€å¤š**æŒ‘é¸ {self.config['clips']['count']} å€‹ã€Œçµ•å°å¯ç”¨ã€çš„ä¸»é¡Œæ®µè½ã€‚

**æ ¸å¿ƒåŸå‰‡ï¼š{style.get('quality_control', {}).get('principle', 'å¯§ç¼ºå‹¿æ¿«')}**
*   å¦‚æœæ‰¾ä¸åˆ°å®Œç¾çš„ç‰‡æ®µï¼Œ**è«‹å›å‚³è¼ƒå°‘çš„æ•¸é‡ï¼Œç”šè‡³ä¸å›å‚³**ã€‚
*   ä¸è¦ç‚ºäº†æ¹Šæ•¸è€Œé¸æ“‡å“è³ªæ™®é€šçš„æ®µè½ã€‚
*   ä¸è¦é¸æ“‡æ•˜äº‹ä¸å®Œæ•´ã€æˆ–åŒ…å«ç¦èªçš„æ®µè½ã€‚

æ¯å€‹æ®µè½å¿…é ˆï¼š
1. æœ‰æ˜ç¢ºçš„é–‹å§‹å’ŒçµæŸ
2. åŒ…å«å®Œæ•´çš„è«–è¿°ï¼ˆæœ‰å‰å› å¾Œæœï¼‰
3. èƒ½ç¨ç«‹ç†è§£ï¼Œä¸éœ€è¦é¡å¤–ä¸Šä¸‹æ–‡
4. è‡³å°‘ {self.config['clips']['min_topic_duration']} ç§’

## è¼¸å‡ºæ ¼å¼

è«‹å‹™å¿…å›å‚³ã€Œç´” JSON æ ¼å¼ã€ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
    {{
        "topic_name": "ä¸»é¡Œåç¨±ï¼ˆå¸ç›æ¨™é¡Œï¼‰",
        "start_text": "ç‰‡æ®µé–‹å§‹çš„ç²¾ç¢ºèªå¥ï¼ˆâš ï¸ å¿…é ˆå®Œå…¨ç¬¦åˆé€å­—ç¨¿ï¼Œé€£æ¨™é»ç¬¦è™Ÿéƒ½ä¸è¦æ”¹ï¼‰",
        "end_text": "ç‰‡æ®µçµæŸçš„ç²¾ç¢ºèªå¥ï¼ˆâš ï¸ å¿…é ˆå®Œå…¨ç¬¦åˆé€å­—ç¨¿ï¼Œé€£æ¨™é»ç¬¦è™Ÿéƒ½ä¸è¦æ”¹ï¼‰",
        "content_summary": "é€™æ®µåœ¨è¬›ä»€éº¼ï¼Ÿï¼ˆ2-3 å¥è©±ï¼Œå…·é«”èªªæ˜å…§å®¹ï¼‰",
        "key_point": "é€™æ®µçš„æ ¸å¿ƒé‡é»æ˜¯ä»€éº¼ï¼Ÿï¼ˆ1 å¥è©±ï¼‰",
        "marketing_angle": "é€™æ®µç¬¦åˆå“ªå€‹è¡ŒéŠ·è¦ç´ ï¼Ÿï¼ˆç—›é»/é‰¤å­/æƒ…ç·’ï¼‰",
        "narrative_check": "é€™æ®µçš„æ•…äº‹æ˜¯å¦å®Œæ•´ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰",
        "why_selected": "ç‚ºä»€éº¼é€™æ®µé©åˆåšè¡ŒéŠ·ç´ æï¼Ÿ",
        "estimated_duration": 120
    }}
]

## é‡è¦æé†’

âš ï¸ **çµ•å°ç¦æ­¢**ï¼š
1. ç¦æ­¢æé€ é€å­—ç¨¿ä¸­æ²’æœ‰çš„å¥å­
2. ç¦æ­¢ä¿®æ”¹åŸæ–‡
3. **ç¦æ­¢åˆ‡åˆ°ä¸€åŠ**ï¼ˆé€™æ˜¯æœ€åš´é‡çš„éŒ¯èª¤ï¼Œå‹™å¿…ç¢ºä¿èªæ„çµæŸï¼‰
4. ç¦æ­¢é¸æ“‡å¹³æ·¡ç„¡å¥‡çš„éå ´

â›”ï¸ **æ ¸å¿ƒå…§å®¹è¿´é¿ï¼ˆè¡ŒéŠ·ç”¨é€”ï¼Œå‹¿æ´©æ¼ä»˜è²»ä¹¾è²¨ï¼‰**ï¼š
è«‹é¿é–‹åŒ…å«ä»¥ä¸‹ã€Œèª²ç¨‹æ ¸å¿ƒå°ˆæœ‰åè©ã€çš„æ®µè½ï¼Œé™¤éæ˜¯ç´”æ•…äº‹åˆ†äº«æˆ–å¼•ç™¼å¥½å¥‡çš„æå•ï¼š
{constraints_str}

âœ… **å„ªå…ˆé¸æ“‡**ï¼š
{positive_str}

## é€å­—ç¨¿å…§å®¹

{text[:100000]}"""
        
        # Try each API key until one works
        for i, api_key in enumerate(self.api_keys, 1):
            try:
                client = Anthropic(api_key=api_key)
                print(f"   å˜—è©¦ API Key #{i}...")
                
                message = client.messages.create(
                    model=self.config['llm']['model'],
                    max_tokens=8192,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                
                json_str = message.content[0].text.strip()
                if json_str.startswith("```json"):
                    json_str = json_str[7:-3]
                elif json_str.startswith("```"):
                    json_str = json_str[3:-3]
                    
                print(f"   âœ… API Key #{i} æˆåŠŸ")
                return json.loads(json_str)
                
            except Exception as e:
                error_msg = str(e)
                if "429" in error_msg or "rate_limit" in error_msg.lower():
                    print(f"   âš ï¸  API Key #{i} é”åˆ°é€Ÿç‡é™åˆ¶")
                    if i < len(self.api_keys):
                        print(f"   ğŸ”„ åˆ‡æ›åˆ°ä¸‹ä¸€å€‹ Key...")
                        continue
                else:
                    print(f"   âŒ API Key #{i} ç™¼ç”ŸéŒ¯èª¤: {e}")
                    if i < len(self.api_keys):
                        continue
        
        print("âŒ LLM åˆ†æå¤±æ•—: æ‰€æœ‰ API Keys éƒ½ç„¡æ³•ä½¿ç”¨")
        return []
    
    def _analyze_with_gemini(self, text):
        """ä½¿ç”¨ Gemini API åˆ†æ"""
        import google.generativeai as genai
        
        model = genai.GenerativeModel(self.config['llm']['model'])
        
        # Construct Prompt from Style Guide
        style = self.style_guide
        
        hooks_str = ""
        for hook in style.get('hooks', []):
            hooks_str += f"    *   **{hook['category']}**ï¼š\n"
            for ex in hook['examples']:
                hooks_str += f"        - {ex}\n"
                
        constraints_str = "\n".join([f"- {item}" for item in style.get('negative_constraints', {}).get('items', [])])
        positive_str = "\n".join([f"{i+1}. {item}" for i, item in enumerate(style.get('positive_examples', {}).get('items', []))])
        
        prompt = f"""
        ## åˆ†æç›®æ¨™ï¼šè¡ŒéŠ·çŸ­å½±éŸ³ç´ æï¼ˆTeasersï¼‰

        æˆ‘å€‘éœ€è¦çš„æ˜¯ã€Œè²©å”®èª²ç¨‹ã€çš„è¡ŒéŠ·ç´ æã€‚é¢¨æ ¼åƒè€ƒã€Œ{style.get('marketing_strategy', {}).get('style_reference', 'å‚‘æ‰£èŠæºé€š')}ã€ã€‚
        ç­–ç•¥æ˜¯**ã€Œ{style.get('marketing_strategy', {}).get('goal', '')}ã€**ã€‚
        è«‹æ‰¾å‡ºèƒ½å¼•ç™¼è§€çœ¾ã€Œ{style.get('marketing_strategy', {}).get('target_audience_feeling', '')}ã€çš„å®Œæ•´æ®µè½ã€‚

        ## æŒ‘é¸æ¨™æº–ï¼ˆä¸‰è¦ç´ ï¼‰

        è«‹å°‹æ‰¾åŒ…å«ä»¥ä¸‹ä»»ä¸€ï¼ˆæˆ–å¤šå€‹ï¼‰è¦ç´ çš„æ®µè½ï¼š
        1.  **ç—›é» (Pain Points)**ï¼šå¸¸è¦‹çš„æºé€šç½é›£ã€‚
        2.  **é‰¤å­ (Hooks) - åƒè€ƒçˆ†æ¬¾é‚è¼¯**ï¼š
        {hooks_str}
        3.  **æƒ…ç·’ (Emotion)**ï¼šèƒ½æŒ‘å‹•æƒ…ç·’çš„æ®µè½ã€‚

        ## é—œéµè¦æ±‚ï¼šæ•˜äº‹å®Œæ•´æ€§ (Narrative Completeness)

        *   **æœ€é‡è¦**ï¼šæ¯å€‹ç‰‡æ®µå¿…é ˆæ˜¯**ã€Œæœ‰é ­æœ‰å°¾çš„å®Œæ•´æ•˜äº‹ã€**ã€‚
        *   **ä¸é™æ™‚é•·**ï¼šåªè¦æ•…äº‹å®Œæ•´ï¼Œé•·åº¦ä¸é™ï¼ˆä½†é€šå¸¸å»ºè­°åœ¨ 30ç§’ åˆ° 3åˆ†é˜ä¹‹é–“ï¼‰ã€‚
        *   **çµæ§‹å®Œæ•´**ï¼šå¿…é ˆåŒ…å«ã€Œæƒ…å¢ƒ/å•é¡Œ -> ç™¼å±•/è½‰æŠ˜ -> æš«æ™‚çš„çµè«–/æ‡¸å¿µã€ã€‚ä¸è¦åˆ‡åœ¨è©±è¬›ä¸€åŠçš„åœ°æ–¹ã€‚

        **ç¬¬äºŒæ­¥ï¼šä¸»é¡Œåˆ‡åˆ†ï¼ˆ{style.get('quality_control', {}).get('principle', 'å¯§ç¼ºå‹¿æ¿«')}ï¼‰**
        è«‹**æœ€å¤š**æŒ‘é¸ {self.config['clips']['count']} å€‹ã€Œçµ•å°å¯ç”¨ã€çš„ä¸»é¡Œæ®µè½ã€‚

        **æ ¸å¿ƒåŸå‰‡ï¼š{style.get('quality_control', {}).get('principle', 'å¯§ç¼ºå‹¿æ¿«')}**
        *   å¦‚æœæ‰¾ä¸åˆ°å®Œç¾çš„ç‰‡æ®µï¼Œ**è«‹å›å‚³è¼ƒå°‘çš„æ•¸é‡ï¼Œç”šè‡³ä¸å›å‚³**ã€‚
        *   ä¸è¦ç‚ºäº†æ¹Šæ•¸è€Œé¸æ“‡å“è³ªæ™®é€šçš„æ®µè½ã€‚
        *   ä¸è¦é¸æ“‡æ•˜äº‹ä¸å®Œæ•´ã€æˆ–åŒ…å«ç¦èªçš„æ®µè½ã€‚

        æ¯å€‹æ®µè½å¿…é ˆï¼š
        1. æœ‰æ˜ç¢ºçš„é–‹å§‹å’ŒçµæŸ
        2. åŒ…å«å®Œæ•´çš„è«–è¿°ï¼ˆæœ‰å‰å› å¾Œæœï¼‰
        3. èƒ½ç¨ç«‹ç†è§£ï¼Œä¸éœ€è¦é¡å¤–ä¸Šä¸‹æ–‡
        4. è‡³å°‘ {self.config['clips']['min_topic_duration']} ç§’ï¼ˆé¿å…éçŸ­çš„ç‰‡æ®µï¼‰

        ## è¼¸å‡ºæ ¼å¼

        è«‹å‹™å¿…å›å‚³ã€Œç´” JSON æ ¼å¼ã€ï¼ˆä¸è¦ markdown code blockï¼‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        [
            {{
                "topic_name": "ä¸»é¡Œåç¨±ï¼ˆå¸ç›æ¨™é¡Œï¼‰",
                "start_text": "ç‰‡æ®µé–‹å§‹çš„ç²¾ç¢ºèªå¥ï¼ˆâš ï¸ å¿…é ˆå®Œå…¨ç¬¦åˆé€å­—ç¨¿ï¼Œé€£æ¨™é»ç¬¦è™Ÿéƒ½ä¸è¦æ”¹ï¼‰",
                "end_text": "ç‰‡æ®µçµæŸçš„ç²¾ç¢ºèªå¥ï¼ˆâš ï¸ å¿…é ˆå®Œå…¨ç¬¦åˆé€å­—ç¨¿ï¼Œé€£æ¨™é»ç¬¦è™Ÿéƒ½ä¸è¦æ”¹ï¼‰",
                "content_summary": "é€™æ®µåœ¨è¬›ä»€éº¼ï¼Ÿï¼ˆ2-3 å¥è©±ï¼Œå…·é«”èªªæ˜å…§å®¹ï¼‰",
                "key_point": "é€™æ®µçš„æ ¸å¿ƒé‡é»æ˜¯ä»€éº¼ï¼Ÿï¼ˆ1 å¥è©±ï¼‰",
                "marketing_angle": "é€™æ®µç¬¦åˆå“ªå€‹è¡ŒéŠ·è¦ç´ ï¼Ÿï¼ˆç—›é»/é‰¤å­/æƒ…ç·’ï¼‰",
                "narrative_check": "é€™æ®µçš„æ•…äº‹æ˜¯å¦å®Œæ•´ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰",
                "why_selected": "ç‚ºä»€éº¼é€™æ®µé©åˆåšè¡ŒéŠ·ç´ æï¼Ÿ",
                "estimated_duration": 120
            }}
        ]

        ## é‡è¦æé†’

        âš ï¸ **çµ•å°ç¦æ­¢**ï¼š
        1. ç¦æ­¢æé€ é€å­—ç¨¿ä¸­æ²’æœ‰çš„å¥å­
        2. ç¦æ­¢ä¿®æ”¹åŸæ–‡
        3. **ç¦æ­¢åˆ‡åˆ°ä¸€åŠ**ï¼ˆé€™æ˜¯æœ€åš´é‡çš„éŒ¯èª¤ï¼Œå‹™å¿…ç¢ºä¿èªæ„çµæŸï¼‰
        4. ç¦æ­¢é¸æ“‡å¹³æ·¡ç„¡å¥‡çš„éå ´

        â›”ï¸ **æ ¸å¿ƒå…§å®¹è¿´é¿ï¼ˆè¡ŒéŠ·ç”¨é€”ï¼Œå‹¿æ´©æ¼ä»˜è²»ä¹¾è²¨ï¼‰**ï¼š
        è«‹é¿é–‹åŒ…å«ä»¥ä¸‹ã€Œèª²ç¨‹æ ¸å¿ƒå°ˆæœ‰åè©ã€çš„æ®µè½ï¼Œé™¤éæ˜¯ç´”æ•…äº‹åˆ†äº«æˆ–å¼•ç™¼å¥½å¥‡çš„æå•ï¼š
        {constraints_str}

        âœ… **å„ªå…ˆé¸æ“‡**ï¼š
        {positive_str}

        ## é€å­—ç¨¿å…§å®¹

        {text[:40000]}
        """
        
        # Try each API key until one works
        for i, api_key in enumerate(self.api_keys, 1):
            try:
                genai.configure(api_key=api_key)
                print(f"   å˜—è©¦ API Key #{i}...")
                
                response = model.generate_content(prompt)
                json_str = response.text.strip()
                if json_str.startswith("```json"):
                    json_str = json_str[7:-3]
                elif json_str.startswith("```"):
                    json_str = json_str[3:-3]
                    
                print(f"   âœ… API Key #{i} æˆåŠŸ")
                return json.loads(json_str)
                
            except Exception as e:
                error_msg = str(e)
                if "429" in error_msg or "quota" in error_msg.lower():
                    print(f"   âš ï¸  API Key #{i} é¡åº¦å·²æ»¿")
                    if i < len(self.api_keys):
                        print(f"   ğŸ”„ åˆ‡æ›åˆ°ä¸‹ä¸€å€‹ Key...")
                        continue
                    else:
                        print(f"   âŒ æ‰€æœ‰ API Keys éƒ½å·²é”åˆ°é¡åº¦ä¸Šé™")
                else:
                    print(f"   âŒ API Key #{i} ç™¼ç”ŸéŒ¯èª¤: {e}")
                    if i < len(self.api_keys):
                        continue
        
        print("âŒ LLM åˆ†æå¤±æ•—: æ‰€æœ‰ API Keys éƒ½ç„¡æ³•ä½¿ç”¨")
        return []

    def _normalize_text(self, text):
        """æ¨™æº–åŒ–æ–‡å­—ä»¥é€²è¡Œæ¨¡ç³Šæ¯”å°"""
        import re
        text = re.sub(r'[^\w\s]', '', text)
        text = text.lower()
        return "".join(text.split())

    def find_timecodes(self, srt_path, clips):
        """åœ¨ SRT ä¸­å°‹æ‰¾å°æ‡‰çš„æ™‚é–“ç¢¼"""
        import re
        from datetime import datetime, timedelta
        
        def parse_time(t_str):
            t_str = t_str.replace(',', '.')
            return datetime.strptime(t_str, "%H:%M:%S.%f")
            
        def format_time(dt):
            # æ ¼å¼åŒ–å› ffmpeg å¯ç”¨çš„å­—ä¸² (HH:MM:SS.mmm)
            return dt.strftime("%H:%M:%S.%f")[:-3]

        def time_diff_sec(t1_str, t2_str):
            t1 = parse_time(t1_str)
            t2 = parse_time(t2_str)
            return (t2 - t1).total_seconds()

        with open(srt_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        import opencc
        cc = opencc.OpenCC('s2t')
        
        blocks = re.split(r'\n\n', content.strip())
        srt_data = []
        full_text_normalized = ""
        
        for block in blocks:
            lines = block.split('\n')
            if len(lines) >= 3:
                time_line = lines[1]
                text = " ".join(lines[2:])
                try:
                    start, end = time_line.split(' --> ')
                    start = start.replace(',', '.')
                    end = end.replace(',', '.')
                except ValueError:
                    continue
                
                text = cc.convert(text)
                norm_text = self._normalize_text(text)
                start_idx = len(full_text_normalized)
                full_text_normalized += norm_text
                end_idx = len(full_text_normalized)
                
                srt_data.append({
                    'start': start, 
                    'end': end, 
                    'text': text,
                    'norm_text': norm_text,
                    'global_start_idx': start_idx,
                    'global_end_idx': end_idx
                })

        results = []
        padding = self.config['clips'].get('padding', 0)
        
        for clip in clips:
            topic_name = clip.get('topic_name', clip.get('topic', 'unknown'))
            print(f"ğŸ” å°‹æ‰¾ä¸»é¡Œ: {topic_name}")
            
            start_norm = self._normalize_text(clip['start_text'])
            end_norm = self._normalize_text(clip['end_text'])
            
            start_pos = full_text_normalized.find(start_norm)
            if start_pos == -1:
                start_pos = full_text_normalized.find(start_norm[:30])
            
            if start_pos == -1:
                print(f"   âš ï¸  æ‰¾ä¸åˆ°é–‹å§‹èªå¥: {clip['start_text'][:20]}...")
                continue
                
            end_pos = full_text_normalized.find(end_norm, start_pos)
            if end_pos == -1:
                end_pos = full_text_normalized.find(end_norm[-30:], start_pos)
                
            if end_pos == -1:
                print(f"   âš ï¸  æ‰¾ä¸åˆ°çµæŸèªå¥: {clip['end_text'][-20:]}...")
                continue

            start_time_str = None
            end_time_str = None
            
            for item in srt_data:
                if start_time_str is None and item['global_end_idx'] > start_pos:
                    start_time_str = item['start']
                if item['global_start_idx'] <= end_pos + len(end_norm):
                    end_time_str = item['end']
            
            if start_time_str and end_time_str:
                # Apply Padding
                t1 = parse_time(start_time_str)
                t2 = parse_time(end_time_str)
                
                # Add padding
                t1 = t1 - timedelta(seconds=padding)
                t2 = t2 + timedelta(seconds=padding)
                
                # Ensure start is not negative (using arbitrary base date 1900-01-01)
                if t1.year < 1900: 
                    t1 = t1.replace(year=1900, month=1, day=1, hour=0, minute=0, second=0, microsecond=0)
                
                duration = (t2 - t1).total_seconds()
                
                if duration < 5:
                    print(f"   âš ï¸  ç‰‡æ®µéçŸ­ ({duration}s)ï¼Œå¿½ç•¥")
                    continue
                
                final_start = format_time(t1)
                final_end = format_time(t2)
                    
                results.append({
                    'start': final_start,
                    'end': final_end,
                    'topic': clip.get('topic_name', clip.get('topic', 'unknown')),
                    'content_summary': clip.get('content_summary', ''),
                    'key_point': clip.get('key_point', ''),
                    'why_selected': clip.get('why_selected', clip.get('reason', '')),
                    'estimated_duration': clip.get('estimated_duration', duration)
                })
                print(f"   âœ… é–å®šæ™‚é–“: {final_start} - {final_end} ({duration:.1f}s, padding={padding}s)")
            else:
                print(f"   âš ï¸  æ™‚é–“ç¢¼å°æ‡‰å¤±æ•—")
                
        return results

    def slice_video(self, video_path, clips, output_dir, mode="proxy"):
        """ä½¿ç”¨ ffmpeg åˆ‡å‰²å½±ç‰‡ (æ”¯æ´ proxy/master æ¨¡å¼)"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        base_name = Path(video_path).stem
        
        # å„²å­˜ metadata
        metadata_path = os.path.join(output_dir, "clips.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(clips, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ’¾ å·²å„²å­˜å‰ªè¼¯è³‡è¨Š: {metadata_path}")
        
        for i, clip in enumerate(clips, 1):
            output_filename = self.config['output']['filename_pattern'].format(
                original_name=base_name,
                index=i,
                topic=clip['topic']
            )
            output_path = os.path.join(output_dir, output_filename)
            
            print(f"âœ‚ï¸  æ­£åœ¨åˆ‡å‰²ç‰‡æ®µ {i} ({mode}): {clip['topic']}")
            
            if mode == "proxy":
                # Proxy æ¨¡å¼ï¼šè½‰ç¢¼ç‚º 720p H.264ï¼Œæª”æ¡ˆå°ï¼Œé©åˆé è¦½
                cmd = [
                    "ffmpeg", "-y",
                    "-ss", clip['start'],
                    "-to", clip['end'],
                    "-i", video_path,
                    "-vf", "scale=-1:720",  # ç¸®æ”¾è‡³ 720p
                    "-c:v", "libx264",      # ä½¿ç”¨ H.264 ç·¨ç¢¼
                    "-crf", "23",           # å¹³è¡¡ç•«è³ªèˆ‡å¤§å°
                    "-preset", "fast",      # å¿«é€Ÿç·¨ç¢¼
                    "-c:a", "aac",          # éŸ³è¨Šè½‰ç¢¼
                    "-avoid_negative_ts", "1",
                    output_path
                ]
            else:
                # Master æ¨¡å¼ï¼šç„¡æè¤‡è£½ï¼Œç•«è³ªæœ€é«˜ï¼Œæª”æ¡ˆå¤§
                cmd = [
                    "ffmpeg", "-y",
                    "-ss", clip['start'],
                    "-to", clip['end'],
                    "-i", video_path,
                    "-c:v", "copy",         # è¦–è¨Šç„¡æè¤‡è£½
                    "-c:a", "copy",         # éŸ³è¨Šç„¡æè¤‡è£½
                    "-avoid_negative_ts", "1",
                    output_path
                ]
            
            try:
                subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                print(f"âœ… å·²å„²å­˜: {output_filename}")
            except subprocess.CalledProcessError as e:
                print(f"âŒ åˆ‡å‰²å¤±æ•—: {e}")

def main():
    import argparse
    parser = argparse.ArgumentParser(description="AI Video Slicer")
    parser.add_argument("--video", required=True, help="å½±ç‰‡æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--srt", required=True, help="å­—å¹•æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--output", default="highlights", help="è¼¸å‡ºç›®éŒ„")
    parser.add_argument("--mode", default="proxy", choices=["proxy", "master"], help="è¼¸å‡ºæ¨¡å¼")
    args = parser.parse_args()
    
    slicer = VideoSlicer()
    
    print(f"ğŸ“– è®€å–å­—å¹•: {args.srt}")
    transcript_text = slicer.parse_srt(args.srt)
    
    clips_info = slicer.analyze_transcript(transcript_text)
    print(f"ğŸ” AI æŒ‘é¸äº† {len(clips_info)} å€‹ç‰‡æ®µ")
    
    clips_with_time = slicer.find_timecodes(args.srt, clips_info)
    
    if clips_with_time:
        print(f"ğŸ¬ é–‹å§‹åˆ‡å‰²å½±ç‰‡ ({args.mode} æ¨¡å¼)...")
        slicer.slice_video(args.video, clips_with_time, args.output, mode=args.mode)
        print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    else:
        print("âŒ ç„¡æ³•æå–ä»»ä½•ç‰‡æ®µ")

if __name__ == "__main__":
    main()
